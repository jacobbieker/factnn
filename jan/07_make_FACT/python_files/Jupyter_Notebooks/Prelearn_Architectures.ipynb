{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import of every needed library\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import h5py\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolders(model_name, save_model_path):\n",
    "    # Iterates over all existing models and chooses the right folder to save everything \n",
    "    file_paths = os.listdir(save_model_path)\n",
    "    for path in file_paths:\n",
    "        name = '_' + model_name\n",
    "        if path.endswith(name):\n",
    "            correct_path = path \n",
    "\n",
    "    # Creates missing folders or chooses the right one to append new data to\n",
    "    if 'correct_path' in locals():\n",
    "        folder_path = os.path.join(save_model_path, correct_path)\n",
    "    else:\n",
    "        folder_number = len(os.listdir(save_model_path))+1\n",
    "        folder_path = save_model_path + '/' + str(folder_number) + '_' + model_name\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "        # Creates the csv to save every models performance in\n",
    "        c_count = model_name.count('c')\n",
    "        depth_names = []\n",
    "        for i in range(c_count):\n",
    "            depth_names.append('Depth_{}'.format(i+1))\n",
    "        columns = ['Learning_Rate','Batch_Size','Patch_Size']\n",
    "        columns.extend(depth_names)\n",
    "        columns.extend(['Accuracy','Auc','Pretraining_Steps', 'Title'])\n",
    "\n",
    "        with open(os.path.join(folder_path, model_name+'_Hyperparameter.csv'), 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(columns)\n",
    "            \n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metaYielder(path_mc_images):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        keys = list(f.keys())\n",
    "        events = []\n",
    "        for key in keys:\n",
    "            events.append(len(f[key]))\n",
    "            \n",
    "    gamma_anteil = events[0]/np.sum(events)\n",
    "    hadron_anteil = events[1]/np.sum(events)\n",
    "    \n",
    "    gamma_count = int(round(num_events*gamma_anteil))\n",
    "    hadron_count = int(round(num_events*hadron_anteil))\n",
    "    \n",
    "    return gamma_anteil, hadron_anteil, gamma_count, hadron_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchYielder(path_mc_images):\n",
    "    gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder(path_mc_images)\n",
    "\n",
    "    gamma_batch_size = int(round(batch_size*gamma_anteil))\n",
    "    hadron_batch_size = int(round(batch_size*hadron_anteil))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        gamma_offset = (step * gamma_batch_size) % (gamma_count - gamma_batch_size)\n",
    "        hadron_offset = (step * hadron_batch_size) % (hadron_count - hadron_batch_size)\n",
    "\n",
    "        with h5py.File(path_mc_images, 'r') as f:\n",
    "            gamma_data = f['Gamma'][gamma_offset:(gamma_offset + gamma_batch_size), :, :, :]\n",
    "            hadron_data = f['Hadron'][hadron_offset:(hadron_offset + hadron_batch_size), :, :, :]\n",
    "\n",
    "        batch_data = np.concatenate((gamma_data, hadron_data), axis=0)\n",
    "        labels = np.array([True]*gamma_batch_size+[False]*hadron_batch_size)\n",
    "        batch_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count):\n",
    "    with h5py.File(path_mc_images, 'r') as f:\n",
    "        gamma_size = int(round(events_in_validation_and_testing*gamma_anteil))\n",
    "        hadron_size = int(round(events_in_validation_and_testing*hadron_anteil))\n",
    "\n",
    "        gamma_valid_data = f['Gamma'][gamma_count:(gamma_count+gamma_size), :, :, :]\n",
    "        hadron_valid_data = f['Hadron'][hadron_count:(hadron_count+hadron_size), :, :, :]\n",
    "\n",
    "        valid_dataset = np.concatenate((gamma_valid_data, hadron_valid_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        valid_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "\n",
    "        gamma_test_data = f['Gamma'][(gamma_count+gamma_size):(gamma_count+2*gamma_size), :, :, :]\n",
    "        hadron_test_data = f['Hadron'][(hadron_count+hadron_size):(hadron_count+2*hadron_size), :, :, :]\n",
    "\n",
    "        test_dataset = np.concatenate((gamma_test_data, hadron_test_data), axis=0)\n",
    "        labels = np.array([True]*gamma_size+[False]*hadron_size)\n",
    "        test_labels = (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "        \n",
    "    return valid_dataset, valid_labels, test_dataset, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestAuc(folder_path, architecture):\n",
    "    # Loading the existing runs to find the best auc untill now. Only a model with a better auc will be saved\n",
    "    df = pd.read_csv(os.path.join(folder_path, architecture+'_Hyperparameter.csv'))\n",
    "    if len(df['Auc']) > 0:\n",
    "        best_auc = df['Auc'].max()\n",
    "    else:\n",
    "        best_auc = 0\n",
    "        \n",
    "    return best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHyperparameter(architecture, number_of_nets):\n",
    "    # Hyperparameter for the model (fit manually)\n",
    "    num_labels = 2 # gamma or proton\n",
    "    num_channels = 1 # it is a greyscale image\n",
    "    \n",
    "    num_steps = 20001     # Maximum batches for the model\n",
    "    \n",
    "    min_batch_size = 64   # How many images will be in a batch\n",
    "    max_batch_size = 257\n",
    "    \n",
    "    patch_size = [3, 5]   # Will the kernel/patch be 3x3 or 5x5\n",
    "\n",
    "    min_depth = 2         # Setting the depth of the convolution layers. New layers will be longer than the preceding\n",
    "    max_depth = 21\n",
    "    \n",
    "    min_num_hidden = 8    # Number of hidden nodes in f-layers. all f-layers will have the same number of nodes\n",
    "    max_num_hidden = 257\n",
    "    \n",
    "    \n",
    "    num_steps = [num_steps] * number_of_nets\n",
    "    batch_size = np.random.randint(min_batch_size, max_batch_size, size=number_of_nets)\n",
    "    patch_size = np.random.choice(patch_size, size=number_of_nets)\n",
    "    layer = architecture[:-1]\n",
    "\n",
    "    depth = []\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets)) # 2 - 21\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[0])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[1])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[2])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[3])\n",
    "    if layer and layer[0]=='c':\n",
    "        layer = layer[1:]\n",
    "        depth.append(np.random.randint(min_depth, max_depth, size=number_of_nets) + depth[4])\n",
    "\n",
    "    num_hidden = np.random.randint(min_num_hidden, max_num_hidden, size=number_of_nets)\n",
    "    \n",
    "    # Combining the hyperparameters to fit them into a for-loop\n",
    "    hyperparameter = zip(num_steps, batch_size, patch_size, zip(*depth), num_hidden)\n",
    "    \n",
    "    return num_labels, num_channels, hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSessConf(per_process_gpu_memory_fraction = 0.1, op_parallelism_threads = 18):\n",
    "    gpu_config = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=per_process_gpu_memory_fraction)\n",
    "    session_conf = tf.ConfigProto(gpu_options=gpu_config, intra_op_parallelism_threads=op_parallelism_threads, inter_op_parallelism_threads=op_parallelism_threads)\n",
    "    \n",
    "    return session_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initWeightsBiases():\n",
    "    # Maximal 6 Convolution Layers & 5 Fully Connectd Layers\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    conv_size_dict={1:23*23, 2:12*12, 3:6*6, 4:3*3, 5:2*2, 6:1*1}\n",
    "    with tf.Session(config=getSessConf()) as sess:\n",
    "        weights_biases = []\n",
    "\n",
    "        if len(depth)>=1:\n",
    "            conv2d_1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth[0]], stddev=0.1), name='W')\n",
    "            conv2d_1_biases = tf.Variable(tf.constant(1.0, shape=[depth[0]]), name='B')\n",
    "\n",
    "        if len(depth)>=2:\n",
    "            conv2d_2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[0], depth[1]], stddev=0.1), name='W')\n",
    "            conv2d_2_biases = tf.Variable(tf.constant(1.0, shape=[depth[1]]), name='B')\n",
    "\n",
    "        if len(depth)>=3:\n",
    "            conv2d_3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[1], depth[2]], stddev=0.1), name='W')\n",
    "            conv2d_3_biases = tf.Variable(tf.constant(1.0, shape=[depth[2]]), name='B')\n",
    "\n",
    "        if len(depth)>=4:\n",
    "            conv2d_4_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[2], depth[3]], stddev=0.1), name='W')\n",
    "            conv2d_4_biases = tf.Variable(tf.constant(1.0, shape=[depth[3]]), name='B')\n",
    "\n",
    "        if len(depth)>=5:\n",
    "            conv2d_5_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[3], depth[4]], stddev=0.1), name='W')\n",
    "            conv2d_5_biases = tf.Variable(tf.constant(1.0, shape=[depth[4]]), name='B')\n",
    "\n",
    "        if len(depth)>=6:\n",
    "            conv2d_6_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth[4], depth[5]], stddev=0.1), name='W')\n",
    "            conv2d_6_biases = tf.Variable(tf.constant(1.0, shape=[depth[5]]), name='B')\n",
    "\n",
    "        shape = conv_size_dict[c_count]*depth[-1]\n",
    "\n",
    "        if f_count-1>=1:\n",
    "            fc_1_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_1_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=2:\n",
    "            fc_2_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=3:\n",
    "            fc_3_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "        if f_count-1>=4:\n",
    "            fc_4_weights = tf.Variable(tf.truncated_normal([shape, num_hidden], stddev=0.1), name='W')\n",
    "            fc_4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name='B')\n",
    "            shape = num_hidden\n",
    "\n",
    "            \n",
    "        # Output Layers\n",
    "        if c_count>=1:\n",
    "            shape = conv_size_dict[1]*depth[0]\n",
    "            out_1_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_1_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=2:\n",
    "            shape = conv_size_dict[2]*depth[1]\n",
    "            out_2_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_2_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=3:\n",
    "            shape = conv_size_dict[3]*depth[2]\n",
    "            out_3_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_3_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=4:\n",
    "            shape = conv_size_dict[4]*depth[3]\n",
    "            out_4_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=5:\n",
    "            shape = conv_size_dict[5]*depth[4]\n",
    "            out_5_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if c_count>=6:\n",
    "            shape = conv_size_dict[6]*depth[5]\n",
    "            out_6_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=1:\n",
    "            shape = num_hidden\n",
    "            out_7_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_7_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=2:\n",
    "            shape = num_hidden\n",
    "            out_8_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_8_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=3:\n",
    "            shape = num_hidden\n",
    "            out_9_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_9_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        if f_count-1>=4:\n",
    "            shape = num_hidden\n",
    "            out_10_weights = tf.Variable(tf.truncated_normal([shape, num_labels], stddev=0.1), name='W')\n",
    "            out_10_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='B')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if len(depth)>=1:\n",
    "            weights_biases.append([conv2d_1_weights.eval(), conv2d_1_biases.eval()])\n",
    "        if len(depth)>=2:\n",
    "            weights_biases.append([conv2d_2_weights.eval(), conv2d_2_biases.eval()])\n",
    "        if len(depth)>=3:\n",
    "            weights_biases.append([conv2d_3_weights.eval(), conv2d_3_biases.eval()])\n",
    "        if len(depth)>=4:\n",
    "            weights_biases.append([conv2d_4_weights.eval(), conv2d_4_biases.eval()])\n",
    "        if len(depth)>=5:\n",
    "            weights_biases.append([conv2d_5_weights.eval(), conv2d_5_biases.eval()])\n",
    "        if len(depth)>=6:\n",
    "            weights_biases.append([conv2d_6_weights.eval(), conv2d_6_biases.eval()])\n",
    "\n",
    "        if f_count-1>=1:\n",
    "            weights_biases.append([fc_1_weights.eval(), fc_1_biases.eval()])\n",
    "        if f_count-1>=2:\n",
    "            weights_biases.append([fc_2_weights.eval(), fc_2_biases.eval()])\n",
    "        if f_count-1>=3:\n",
    "            weights_biases.append([fc_3_weights.eval(), fc_3_biases.eval()])\n",
    "        if f_count-1>=4:\n",
    "            weights_biases.append([fc_4_weights.eval(), fc_4_biases.eval()])\n",
    "            \n",
    "        if c_count>=1:\n",
    "            weights_biases.append([out_1_weights.eval(), out_1_biases.eval()])\n",
    "        if c_count>=2:\n",
    "            weights_biases.append([out_2_weights.eval(), out_2_biases.eval()])\n",
    "        if c_count>=3:\n",
    "            weights_biases.append([out_3_weights.eval(), out_3_biases.eval()])\n",
    "        if c_count>=4:\n",
    "            weights_biases.append([out_4_weights.eval(), out_4_biases.eval()])\n",
    "        if c_count>=5:\n",
    "            weights_biases.append([out_5_weights.eval(), out_5_biases.eval()])\n",
    "        if c_count>=6:\n",
    "            weights_biases.append([out_6_weights.eval(), out_6_biases.eval()])\n",
    "        if f_count-1>=1:\n",
    "            weights_biases.append([out_7_weights.eval(), out_7_biases.eval()])\n",
    "        if f_count-1>=2:\n",
    "            weights_biases.append([out_8_weights.eval(), out_8_biases.eval()])\n",
    "        if f_count-1>=3:\n",
    "            weights_biases.append([out_9_weights.eval(), out_9_biases.eval()])\n",
    "        if f_count-1>=4:\n",
    "            weights_biases.append([out_01_weights.eval(), out_10_biases.eval()])\n",
    "        \n",
    "\n",
    "        #weights_biases.append([fc_5_weights.eval(), fc_5_biases.eval()])\n",
    "\n",
    "    return weights_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input arguments from outside\n",
    "#path_mc_images = sys.argv[1]\n",
    "#save_model_path = sys.argv[2]\n",
    "\n",
    "path_mc_images = '/fhgfs/users/jbehnken/make_Data/MC_diffuse_flat_preprocessed_images.h5'\n",
    "save_model_path = '/fhgfs/users/jbehnken/crap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " /fhgfs/users/jbehnken/crap/1_ccccccffff\n",
      "\n",
      " 20001 228 5 (6, 21, 32, 39, 51, 64) 15\n",
      "Initialized\n",
      "\n",
      "Pretraining: cf\n",
      "St_auc: 0.0, sc: 0,val: 55.1, Step: 0\n",
      "St_auc: 0.5011324947798437, sc: 0,val: 49.3, Step: 500\n",
      "St_auc: 0.5060036038184577, sc: 0,val: 55.04, Step: 1000\n",
      "St_auc: 0.5178362272622468, sc: 0,val: 54.98, Step: 1500\n",
      "St_auc: 0.5178362272622468, sc: 1,val: 54.98, Step: 2000\n",
      "St_auc: 0.5178362272622468, sc: 2,val: 54.98, Step: 2500\n",
      "St_auc: 0.5178362272622468, sc: 3,val: 54.98, Step: 3000\n",
      "St_auc: 0.5178362272622468, sc: 4,val: 54.98, Step: 3500\n",
      "St_auc: 0.5178362272622468, sc: 5,val: 54.98, Step: 4000\n",
      "St_auc: 0.5178362272622468, sc: 6,val: 54.98, Step: 4500\n",
      "Finale score: 0.5368392440706409\n",
      "\n",
      "Pretraining: ccf\n",
      "St_auc: 0.0, sc: 0,val: 54.96, Step: 0\n",
      "St_auc: 0.5122660947479162, sc: 0,val: 45.06, Step: 500\n",
      "St_auc: 0.5122660947479162, sc: 1,val: 45.06, Step: 1000\n",
      "St_auc: 0.5122660947479162, sc: 2,val: 45.06, Step: 1500\n",
      "St_auc: 0.5122660947479162, sc: 3,val: 45.06, Step: 2000\n",
      "St_auc: 0.5122660947479162, sc: 4,val: 45.06, Step: 2500\n",
      "St_auc: 0.5122660947479162, sc: 5,val: 45.06, Step: 3000\n",
      "St_auc: 0.5122660947479162, sc: 6,val: 45.06, Step: 3500\n",
      "St_auc: 0.5122660947479162, sc: 7,val: 45.06, Step: 4000\n",
      "St_auc: 0.5122660947479162, sc: 8,val: 45.06, Step: 4500\n",
      "Finale score: 0.5341159811025739\n",
      "\n",
      "Pretraining: cccf\n",
      "St_auc: 0.0, sc: 0,val: 54.92, Step: 0\n",
      "St_auc: 0.45952967131475875, sc: 0,val: 44.98, Step: 500\n",
      "St_auc: 0.45952967131475875, sc: 1,val: 44.98, Step: 1000\n",
      "St_auc: 0.45952967131475875, sc: 2,val: 44.98, Step: 1500\n",
      "St_auc: 0.45952967131475875, sc: 3,val: 44.98, Step: 2000\n",
      "St_auc: 0.45952967131475875, sc: 4,val: 44.98, Step: 2500\n",
      "St_auc: 0.45952967131475875, sc: 5,val: 44.98, Step: 3000\n",
      "St_auc: 0.45952967131475875, sc: 6,val: 44.98, Step: 3500\n",
      "St_auc: 0.45952967131475875, sc: 7,val: 44.98, Step: 4000\n",
      "St_auc: 0.45952967131475875, sc: 8,val: 44.98, Step: 4500\n",
      "Finale score: 0.37761769244776733\n",
      "\n",
      "Pretraining: ccccf\n",
      "St_auc: 0.0, sc: 0,val: 54.72, Step: 0\n",
      "St_auc: 0.49649256235790296, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.49649256235790296, sc: 1,val: 54.94, Step: 1000\n",
      "St_auc: 0.49649256235790296, sc: 2,val: 54.94, Step: 1500\n",
      "St_auc: 0.49649256235790296, sc: 3,val: 54.94, Step: 2000\n",
      "St_auc: 0.5333827840434733, sc: 0,val: 45.06, Step: 2500\n",
      "St_auc: 0.5333827840434733, sc: 1,val: 45.06, Step: 3000\n",
      "St_auc: 0.5333827840434733, sc: 2,val: 45.06, Step: 3500\n",
      "St_auc: 0.5333827840434733, sc: 3,val: 45.06, Step: 4000\n",
      "St_auc: 0.5333827840434733, sc: 4,val: 45.06, Step: 4500\n",
      "Finale score: 0.40035096189346536\n",
      "\n",
      "Pretraining: cccccf\n",
      "St_auc: 0.0, sc: 0,val: 55.36, Step: 0\n",
      "St_auc: 0.5255157100729343, sc: 0,val: 45.06, Step: 500\n",
      "St_auc: 0.5255157100729343, sc: 1,val: 45.06, Step: 1000\n",
      "St_auc: 0.5255157100729343, sc: 2,val: 45.06, Step: 1500\n",
      "St_auc: 0.5255157100729343, sc: 3,val: 45.06, Step: 2000\n",
      "St_auc: 0.5255157100729343, sc: 4,val: 45.06, Step: 2500\n",
      "St_auc: 0.5255157100729343, sc: 5,val: 45.06, Step: 3000\n",
      "St_auc: 0.5255157100729343, sc: 6,val: 45.06, Step: 3500\n",
      "St_auc: 0.5255157100729343, sc: 7,val: 45.06, Step: 4000\n",
      "St_auc: 0.5255157100729343, sc: 8,val: 45.06, Step: 4500\n",
      "Finale score: 0.3908904537104675\n",
      "\n",
      "Pretraining: ccccccf\n",
      "St_auc: 0.0, sc: 0,val: 44.98, Step: 0\n",
      "St_auc: 0.5866110889481015, sc: 0,val: 54.98, Step: 500\n",
      "St_auc: 0.5866110889481015, sc: 1,val: 54.98, Step: 1000\n",
      "St_auc: 0.5866110889481015, sc: 2,val: 54.98, Step: 1500\n",
      "St_auc: 0.5866110889481015, sc: 3,val: 54.98, Step: 2000\n",
      "St_auc: 0.5866110889481015, sc: 4,val: 54.98, Step: 2500\n",
      "St_auc: 0.5866110889481015, sc: 5,val: 54.98, Step: 3000\n",
      "St_auc: 0.5866110889481015, sc: 6,val: 54.98, Step: 3500\n",
      "St_auc: 0.5866110889481015, sc: 7,val: 54.98, Step: 4000\n",
      "St_auc: 0.5866110889481015, sc: 8,val: 54.98, Step: 4500\n",
      "Finale score: 0.6021635432980917\n",
      "\n",
      "Pretraining: ccccccff\n",
      "St_auc: 0.0, sc: 0,val: 55.32, Step: 0\n",
      "St_auc: 0.4923170028846382, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.4923170028846382, sc: 1,val: 54.94, Step: 1000\n",
      "St_auc: 0.4923170028846382, sc: 2,val: 54.94, Step: 1500\n",
      "St_auc: 0.5038027200233447, sc: 0,val: 54.94, Step: 2000\n",
      "St_auc: 0.5060613676768959, sc: 0,val: 54.94, Step: 2500\n",
      "St_auc: 0.5060613676768959, sc: 1,val: 54.94, Step: 3000\n",
      "St_auc: 0.5060613676768959, sc: 2,val: 54.94, Step: 3500\n",
      "St_auc: 0.5060613676768959, sc: 3,val: 54.94, Step: 4000\n",
      "St_auc: 0.5060613676768959, sc: 4,val: 54.94, Step: 4500\n",
      "Finale score: 0.6313701215593948\n",
      "\n",
      "Pretraining: ccccccfff\n",
      "St_auc: 0.0, sc: 0,val: 54.94, Step: 0\n",
      "St_auc: 0.48813675767180786, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.5226285674029902, sc: 0,val: 54.94, Step: 1000\n",
      "St_auc: 0.5226285674029902, sc: 1,val: 54.94, Step: 1500\n",
      "St_auc: 0.5226285674029902, sc: 2,val: 54.94, Step: 2000\n",
      "St_auc: 0.5226285674029902, sc: 3,val: 54.94, Step: 2500\n",
      "St_auc: 0.5300474261474932, sc: 0,val: 54.94, Step: 3000\n",
      "St_auc: 0.5300474261474932, sc: 1,val: 54.94, Step: 3500\n",
      "St_auc: 0.5300474261474932, sc: 2,val: 54.94, Step: 4000\n",
      "St_auc: 0.5300474261474932, sc: 3,val: 54.94, Step: 4500\n",
      "Finale score: 0.7367227549692671\n",
      "\n",
      "Pretraining: ccccccffff\n",
      "St_auc: 0.0, sc: 0,val: 45.06, Step: 0\n",
      "St_auc: 0.5253467406884256, sc: 0,val: 54.94, Step: 500\n",
      "St_auc: 0.5253467406884256, sc: 1,val: 54.94, Step: 1000\n",
      "St_auc: 0.5253467406884256, sc: 2,val: 54.94, Step: 1500\n",
      "St_auc: 0.5253467406884256, sc: 3,val: 54.94, Step: 2000\n",
      "St_auc: 0.5253467406884256, sc: 4,val: 54.94, Step: 2500\n",
      "St_auc: 0.5253467406884256, sc: 5,val: 54.94, Step: 3000\n",
      "St_auc: 0.5253467406884256, sc: 6,val: 45.06, Step: 3500\n",
      "St_auc: 0.5253467406884256, sc: 7,val: 54.94, Step: 4000\n",
      "St_auc: 0.5253467406884256, sc: 8,val: 45.06, Step: 4500\n",
      "St_auc: 0.5253467406884256, sc: 9,val: 45.06, Step: 5000\n",
      "St_auc: 0.5253467406884256, sc: 10,val: 45.06, Step: 5500\n",
      "Finale score: 0.2905729302240058\n"
     ]
    }
   ],
   "source": [
    "_print = False\n",
    "\n",
    "pretraining_steps = [5000]\n",
    "\n",
    "# Number of events in training-dataset\n",
    "num_events = 100000\n",
    "\n",
    "# Number of events in validation-/test-dataset\n",
    "events_in_validation_and_testing = 5000\n",
    "\n",
    "# Number of nets to compute\n",
    "number_of_nets = 1\n",
    "\n",
    "# Comment on the run\n",
    "title_name = 'test_test'\n",
    "\n",
    "# Architectures to test\n",
    "test_architectures = ['ccccccffff']\n",
    "\n",
    "\n",
    "\n",
    "gamma_anteil, hadron_anteil, gamma_count, hadron_count = metaYielder(path_mc_images)\n",
    "valid_dataset, valid_labels, test_dataset, test_labels = getValidationTesting(path_mc_images, events_in_validation_and_testing, gamma_anteil, hadron_anteil, gamma_count, hadron_count)\n",
    "\n",
    "\n",
    "for architecture in test_architectures:\n",
    "    c_count = architecture.count('c')\n",
    "    f_count = architecture.count('f')\n",
    "    folder_path = createFolders(architecture, save_model_path)\n",
    "    print('\\n\\n', folder_path)\n",
    "    \n",
    "    best_auc = bestAuc(folder_path, architecture)\n",
    "        \n",
    "    num_labels, num_channels, hyperparameter = getHyperparameter(architecture, number_of_nets)\n",
    "    for pretraining in pretraining_steps:\n",
    "        for num_steps, batch_size, patch_size, depth, num_hidden in hyperparameter:\n",
    "            try:\n",
    "                print('\\n', num_steps, batch_size, patch_size, depth, num_hidden)\n",
    "\n",
    "                weights_biases = initWeightsBiases()\n",
    "                print('Initialized')\n",
    "\n",
    "                for pretraining_step in range(len(architecture)-1):\n",
    "                    print('\\nPretraining: {}{}'.format(architecture[:pretraining_step+1], 'f'))\n",
    "\n",
    "\n",
    "                    # Measuring the loop-time\n",
    "                    start = time.time()\n",
    "                    # Path to logfiles and correct file name\n",
    "                    LOGDIR = '/fhgfs/users/jbehnken/tf_logs/small_logs'\n",
    "                    # Getting the right count-number for the new logfiles\n",
    "                    logcount = str(len(os.listdir(LOGDIR)))\n",
    "                    hparams = '_bs={}_ps={}_d={}_nh={}_ns={}'.format(batch_size, patch_size, depth, num_hidden, num_steps)\n",
    "\n",
    "\n",
    "                    tf.reset_default_graph()\n",
    "                    with tf.Session(config=getSessConf()) as sess:\n",
    "                        # Create tf.variables for the three different datasets\n",
    "                        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 46, 45, num_channels), name='train_data')\n",
    "                        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='train_labels')\n",
    "\n",
    "                        tf_valid_dataset = tf.constant(valid_dataset, name='valid_data')\n",
    "                        tf_valid_labels = tf.constant(valid_labels, name='valid_labels')\n",
    "\n",
    "                        tf_test_dataset_final = tf.constant(test_dataset, name='test_data_final')\n",
    "                        tf_test_labels_final = tf.constant(test_labels, name='test_labels_final')                    \n",
    "\n",
    "                        # Summary for same example input images\n",
    "                        tf.summary.image('input', tf_train_dataset, 6)\n",
    "\n",
    "\n",
    "\n",
    "                        # Creating the graph. Only layers specified in 'model_name' will be added and correctly sized\n",
    "                        layer = architecture[:-1]\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=0:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_1'):\n",
    "                                init_w_1 = tf.constant(weights_biases[0][0])\n",
    "                                layer1_weights = tf.get_variable('W_1', initializer=init_w_1)\n",
    "                                init_b_1 = tf.constant(weights_biases[0][1])\n",
    "                                layer1_biases = tf.get_variable('B_1', initializer=init_b_1)\n",
    "\n",
    "                                conv = tf.nn.conv2d(tf_train_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer1_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer1_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 1 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=1:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_2'):\n",
    "                                init_w_2 = tf.constant(weights_biases[1][0])\n",
    "                                layer2_weights = tf.get_variable('W_2', initializer=init_w_2)\n",
    "                                init_b_2 = tf.constant(weights_biases[1][1])\n",
    "                                layer2_biases = tf.get_variable('B_2', initializer=init_b_2)\n",
    "\n",
    "                                conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer2_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer2_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 2 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=2:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_3'):\n",
    "                                init_w_3 = tf.constant(weights_biases[2][0])\n",
    "                                layer3_weights = tf.get_variable('W_3', initializer=init_w_3)\n",
    "                                init_b_3 = tf.constant(weights_biases[2][1])\n",
    "                                layer3_biases = tf.get_variable('B_3', initializer=init_b_3)\n",
    "\n",
    "                                conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer3_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer3_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer3_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 3 initialized')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=3:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_4'):\n",
    "                                init_w_4 = tf.constant(weights_biases[3][0])\n",
    "                                layer4_weights = tf.get_variable('W_4', initializer=init_w_4)\n",
    "                                init_b_4 = tf.constant(weights_biases[3][1])\n",
    "                                layer4_biases = tf.get_variable('B_4', initializer=init_b_4)\n",
    "\n",
    "                                conv = tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer4_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer4_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer4_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 4 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=4:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_5'):\n",
    "                                init_w_5 = tf.constant(weights_biases[4][0])\n",
    "                                layer5_weights = tf.get_variable('W_5', initializer=init_w_5)\n",
    "                                init_b_5 = tf.constant(weights_biases[4][1])\n",
    "                                layer5_biases = tf.get_variable('B_5', initializer=init_b_5)\n",
    "\n",
    "                                conv = tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer5_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer5_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer5_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 5 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='c' and pretraining_step>=5:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('conv2d_6'):\n",
    "                                init_w_6 = tf.constant(weights_biases[5][0])\n",
    "                                layer6_weights = tf.get_variable('W_6', initializer=init_w_6)\n",
    "                                init_b_6 = tf.constant(weights_biases[5][1])\n",
    "                                layer6_biases = tf.get_variable('B_6', initializer=init_b_6)\n",
    "\n",
    "                                conv = tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')\n",
    "                                hidden = tf.nn.relu(conv + layer6_biases)\n",
    "                                pool = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                                pool = tf.nn.dropout(pool, 0.9)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer6_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer6_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "                                tf.summary.histogram(\"pooling\", pool)\n",
    "\n",
    "                                #print('Conv-Layer 6 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        # Reshape convolution layers to process the nodes further with connected layers\n",
    "                        with tf.name_scope('reshape'):\n",
    "                            shape = pool.get_shape().as_list()\n",
    "                            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                            output = tf.nn.dropout(output, 0.75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='f' and pretraining_step>=c_count:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('fc_7'):\n",
    "                                init_w_7 = tf.constant(weights_biases[c_count][0])\n",
    "                                layer7_weights = tf.get_variable('W_7', initializer=init_w_7)\n",
    "                                init_b_7 = tf.constant(weights_biases[c_count][1])\n",
    "                                layer7_biases = tf.get_variable('B_7', initializer=init_b_7)\n",
    "\n",
    "                                hidden = tf.nn.relu(tf.matmul(output, layer7_weights) + layer7_biases)\n",
    "                                output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer7_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer7_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "                                #print('Fc-Layer 1 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='f' and pretraining_step>=c_count+1:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('fc_8'):\n",
    "                                init_w_8 = tf.constant(weights_biases[c_count+1][0])\n",
    "                                layer8_weights = tf.get_variable('W_8', initializer=init_w_8)\n",
    "                                init_b_8 = tf.constant(weights_biases[c_count+1][1])\n",
    "                                layer8_biases = tf.get_variable('B_8', initializer=init_b_8)\n",
    "\n",
    "                                hidden = tf.nn.relu(tf.matmul(output, layer8_weights) + layer8_biases)\n",
    "                                output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer8_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer8_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "                                #print('Fc-Layer 2 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='f' and pretraining_step>=c_count+2:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('fc_9'):\n",
    "                                init_w_9 = tf.constant(weights_biases[c_count+2][0])\n",
    "                                layer9_weights = tf.get_variable('W_9', initializer=init_w_9)\n",
    "                                init_b_9 = tf.constant(weights_biases[c_count+2][1])\n",
    "                                layer9_biases = tf.get_variable('B_9', initializer=init_b_9)\n",
    "\n",
    "                                hidden = tf.nn.relu(tf.matmul(output, layer9_weights) + layer9_biases)\n",
    "                                output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer9_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer9_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "                                #print('Fc-Layer 3 initialized')\n",
    "\n",
    "\n",
    "\n",
    "                        if layer and layer[0]=='f' and pretraining_step>=c_count+3:\n",
    "                            layer = layer[1:]\n",
    "                            with tf.name_scope('fc_10'):\n",
    "                                init_w_10 = tf.constant(weights_biases[c_count+3][0])\n",
    "                                layer10_weights = tf.get_variable('W_10', initializer=init_w_10)\n",
    "                                init_b_10 = tf.constant(weights_biases[c_count+3][1])\n",
    "                                layer10_biases = tf.get_variable('B_10', initializer=init_b_10)\n",
    "\n",
    "                                hidden = tf.nn.relu(tf.matmul(output, layer10_weights) + layer10_biases)\n",
    "                                output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                                tf.summary.histogram(\"weights\", layer10_weights)\n",
    "                                tf.summary.histogram(\"biases\", layer10_biases)\n",
    "                                tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "                                #print('Fc-Layer 4 initialized')\n",
    "\n",
    "\n",
    "                        with tf.name_scope('output_layer'):\n",
    "                            i = len(architecture)-1-pretraining_step\n",
    "\n",
    "                            #print('\\nReshape-shape:', output.shape)\n",
    "\n",
    "                            init_w_out = tf.constant(weights_biases[-i][0])\n",
    "                            layerout_weights = tf.get_variable('W_out', initializer=init_w_out)\n",
    "\n",
    "                            #print('Outputweights-shape:', init_w_out.shape)\n",
    "                            #print(i) # i or 3\n",
    "                            init_b_out = tf.constant(weights_biases[len(architecture)-1][-1])\n",
    "                            layerout_biases = tf.get_variable('B_out', initializer=init_b_out)\n",
    "\n",
    "                            #print('Outputbiases-shape:', init_b_out.shape, '\\n')\n",
    "\n",
    "                            hidden = tf.nn.relu(tf.matmul(output, layerout_weights) + layerout_biases)\n",
    "                            output = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "                            tf.summary.histogram(\"weights\", layerout_weights)\n",
    "                            tf.summary.histogram(\"biases\", layerout_biases)\n",
    "                            tf.summary.histogram(\"activations\", hidden)\n",
    "\n",
    "                            #print('Output-Layer initialized')\n",
    "\n",
    "\n",
    "                        # Computing the loss of the model\n",
    "                        with tf.name_scope('loss'):\n",
    "                            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf_train_labels), name='loss')\n",
    "                            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "                        # Optimizing the model\n",
    "                        with tf.name_scope('optimizer'):\n",
    "                            optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "                        # Predictions for the training, validation, and test data\n",
    "                        with tf.name_scope('prediction'):\n",
    "                            train_prediction = tf.nn.softmax(output)\n",
    "\n",
    "                        with tf.name_scope('accuracy'):\n",
    "                            correct_prediction = tf.equal(tf.argmax(train_prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "                            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                            tf.summary.scalar('batch_accuracy', accuracy)\n",
    "                        #print('Loss/Optimizer/Prediction/Accuray initiated')\n",
    "\n",
    "\n",
    "\n",
    "                        # Computing the validation-dataset\n",
    "                        with tf.name_scope('validation'):\n",
    "                            layer = architecture[:pretraining_step+1]\n",
    "\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_valid_dataset, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')  + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')  + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')  + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')  + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')  + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            shape = pool.get_shape().as_list()\n",
    "                            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer7_weights) + layer7_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer8_weights) + layer8_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer9_weights) + layer9_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer10_weights) + layer10_biases)\n",
    "                            valid_prediction = tf.nn.softmax(tf.matmul(output, layerout_weights) + layerout_biases)\n",
    "\n",
    "                            correct_prediction = tf.equal(tf.argmax(valid_prediction, 1), tf.argmax(valid_labels, 1))\n",
    "                            valid_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                            tf.summary.scalar('validation_accuracy', valid_accuracy)                        \n",
    "\n",
    "                        with tf.name_scope('auc'):\n",
    "                            valid_auc = tf.metrics.auc(labels=tf_valid_labels, predictions=valid_prediction, curve='ROC')\n",
    "                            tf.summary.scalar('validation_auc_0', valid_auc[0])\n",
    "                            #tf.summary.scalar('validation_auc_1', valid_auc[1])\n",
    "\n",
    "\n",
    "\n",
    "                        # Computing the test-dataset\n",
    "                        with tf.name_scope('testing'):\n",
    "                            layer = architecture[:pretraining_step+1]\n",
    "\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(tf_test_dataset_final, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')  + layer2_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')  + layer3_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer4_weights, [1, 1, 1, 1], padding='SAME')  + layer4_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer5_weights, [1, 1, 1, 1], padding='SAME')  + layer5_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            if layer and layer[0]=='c':\n",
    "                                layer = layer[1:]\n",
    "                                pool = tf.nn.max_pool(tf.nn.relu(tf.nn.conv2d(pool, layer6_weights, [1, 1, 1, 1], padding='SAME')  + layer6_biases), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "                            shape = pool.get_shape().as_list()\n",
    "                            output = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer7_weights) + layer7_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer8_weights) + layer8_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer9_weights) + layer9_biases)\n",
    "                            if layer and layer[0]=='f':\n",
    "                                layer = layer[1:]\n",
    "                                output = tf.nn.relu(tf.matmul(output, layer10_weights) + layer10_biases)\n",
    "                            test_prediction = tf.nn.softmax(tf.matmul(output, layerout_weights) + layerout_biases)\n",
    "\n",
    "                            test_correct_prediction = tf.equal(tf.argmax(test_prediction, 1), tf.argmax(test_labels, 1))\n",
    "                            test_accuracy = tf.reduce_mean(tf.cast(test_correct_prediction, tf.float32))\n",
    "                            tf.summary.scalar('validation_accuracy', test_accuracy)                        \n",
    "\n",
    "                        with tf.name_scope('auc'):\n",
    "                            test_auc = tf.metrics.auc(labels=tf_test_labels_final, predictions=test_prediction, curve='ROC')\n",
    "                            tf.summary.scalar('test_auc_0', test_auc[0])\n",
    "                            #tf.summary.scalar('validation_auc_1', test_auc[1])\n",
    "\n",
    "\n",
    "\n",
    "                        # Merge all summaries and create a saver\n",
    "                        summ = tf.summary.merge_all()\n",
    "                        saver = tf.train.Saver()\n",
    "\n",
    "                        # Initializing the model-variables and specify the logfiles\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        writer = tf.summary.FileWriter(LOGDIR+'/'+logcount+hparams)\n",
    "                        writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "                        # Iterating over num_steps batches and train the model \n",
    "                        gen = batchYielder(path_mc_images)\n",
    "                        \n",
    "                        if pretraining_step+1 < len(architecture)-1:\n",
    "                            steps = pretraining\n",
    "                            early = False\n",
    "                        else:\n",
    "                            steps = num_steps\n",
    "                            early = True\n",
    "                        \n",
    "                        for step in range(steps):\n",
    "                            batch_data, batch_labels = next(gen)\n",
    "                            # Creating a feed_dict to train the model on in this step\n",
    "                            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                            # Train the model for this step\n",
    "                            #_, l, predictions = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                            _ = sess.run([optimizer], feed_dict=feed_dict)\n",
    "\n",
    "                            # Updating the output to stay in touch with the training process\n",
    "                            # Checking for early-stopping with scikit-learn\n",
    "                            if (step % 500 == 0):\n",
    "                                s = sess.run(summ, feed_dict={tf_train_dataset: batch_data, tf_train_labels: batch_labels})\n",
    "                                writer.add_summary(s, step)\n",
    "\n",
    "                                # Compute the accuracy and the roc-auc-score with scikit-learn\n",
    "                                pred = sess.run(valid_prediction)\n",
    "                                pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "                                stop_acc = accuracy_score(np.argmax(valid_labels, axis=1), np.argmax(pred, axis=1))\n",
    "                                stop_auc = roc_auc_score(valid_labels, pred)\n",
    "                                print(stop_auc)\n",
    "\n",
    "\n",
    "                                # Check if early-stopping is necessary\n",
    "                                auc_now = stop_auc\n",
    "                                if step == 0:\n",
    "                                    stopping_auc = 0.0\n",
    "                                    sink_count = 0\n",
    "                                else:\n",
    "                                    if auc_now > stopping_auc:\n",
    "                                        stopping_auc = auc_now\n",
    "                                        sink_count = 0\n",
    "                                        # Check if the model is better than the existing one and has to be saved\n",
    "                                        if stopping_auc > best_auc:\n",
    "                                            saver.save(sess, os.path.join(folder_path, architecture))\n",
    "                                            best_auc = stopping_auc\n",
    "                                    else:\n",
    "                                        sink_count += 1\n",
    "\n",
    "                                # Printing a current evaluation of the model\n",
    "                                print('St_auc: {}, sc: {},val: {}, Step: {}'.format(stopping_auc, sink_count, stop_acc*100, step))\n",
    "                                if sink_count == 10:\n",
    "                                    if early:\n",
    "                                        break   \n",
    "\n",
    "                        # Compute the final score of the model       \n",
    "                        pred = sess.run(test_prediction)\n",
    "                        pred = np.array(list(zip(pred[:,0], pred[:,1])))\n",
    "                        f_acc = accuracy_score(np.argmax(test_labels, axis=1), np.argmax(pred, axis=1))\n",
    "                        f_auc = roc_auc_score(test_labels, pred)\n",
    "                        print('Finale score: {}'.format(f_auc))\n",
    "\n",
    "\n",
    "                        if 'layer1_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[0][0] = layer1_weights.eval()\n",
    "                            weights_biases[0][1] = layer1_biases.eval()\n",
    "                        if 'layer2_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[1][0] = layer2_weights.eval()\n",
    "                            weights_biases[1][1] = layer2_biases.eval()\n",
    "                        if 'layer3_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[2][0] = layer3_weights.eval()\n",
    "                            weights_biases[2][1] = layer3_biases.eval()\n",
    "                        if 'layer4_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[3][0] = layer4_weights.eval()\n",
    "                            weights_biases[3][1] = layer4_biases.eval()\n",
    "                        if 'layer5_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[4][0] = layer5_weights.eval()\n",
    "                            weights_biases[4][1] = layer5_biases.eval()\n",
    "                        if 'layer6_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[5][0] = layer6_weights.eval()\n",
    "                            weights_biases[5][1] = layer6_biases.eval()\n",
    "                        if 'layer7_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[c_count][0] = layer7_weights.eval()\n",
    "                            weights_biases[c_count][1] = layer7_biases.eval()\n",
    "                        if 'layer8_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[c_count+1][0] = layer8_weights.eval()\n",
    "                            weights_biases[c_count+1][1] = layer8_biases.eval()\n",
    "                        if 'layer9_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[c_count+2][0] = layer9_weights.eval()\n",
    "                            weights_biases[c_count+2][1] = layer9_biases.eval()\n",
    "                        if 'layer10_weights' in locals():\n",
    "                            if _print: print('Values bergeben!')\n",
    "                            weights_biases[c_count+3][0] = layer10_weights.eval()\n",
    "                            weights_biases[c_count+3][1] = layer10_biases.eval()\n",
    "                if 'layer1_weights' in locals():\n",
    "                    del layer1_weights, layer1_biases\n",
    "                if 'layer2_weights' in locals():\n",
    "                    del layer2_weights, layer2_biases\n",
    "                if 'layer3_weights' in locals():\n",
    "                    del layer3_weights, layer3_biases\n",
    "                if 'layer4_weights' in locals():\n",
    "                    del layer4_weights, layer4_biases\n",
    "                if 'layer5_weights' in locals():\n",
    "                    del layer5_weights, layer5_biases\n",
    "                if 'layer6_weights' in locals():\n",
    "                    del layer6_weights, layer6_biases\n",
    "                if 'layer7_weights' in locals():\n",
    "                    del layer7_weights, layer7_biases\n",
    "                if 'layer8_weights' in locals():\n",
    "                    del layer8_weights, layer8_biases\n",
    "                if 'layer9_weights' in locals():\n",
    "                    del layer9_weights, layer9_biases\n",
    "                if 'layer10_weights' in locals():\n",
    "                    del layer10_weights, layer10_biases\n",
    "                    \n",
    "                with open(os.path.join(folder_path, architecture+'_Hyperparameter.csv'), 'a') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([f_acc, f_auc, pretraining, architecture])\n",
    "            except:\n",
    "                sess.close()\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
